TO TEST THE MODEL - THE FOLLOWING CONFIG ARE NEEDED TO BE FOLLOWED

#CHANGE THE param.py AS PER THE BEST CONFIG TO MAKE THE MODEL WORK ON THE TEST SET.

## INPUT 
## INPUT 
POSE_AS_INPUT = 'coordinates' #coordinates, graph, picture
BBOX_AS_INPUT = 'new_coordinates' #old_coordinates, new_coordinates, picture
USE_OCR_AS = 'input' #'input' cost_func
SAVE_PATH = 'experiments/'

SA_LOAD_MODEL_PATH = 'best_model/Self_Attention_Model' #POSE_AS_INPUT = "coordinates", BBOX_AS_INPUT='new_coordinates', USE_OCR_AS = 'input' - Runtime error otherwise
CA_LOAD_MODEL_PATH = 'best_model/Hierarchical_Cross_Attention_Model' #POSE_AS_INPUT = "graph", BBOX_AS_INPUT='new_coordinates', USE_OCR_AS = 'input' - Runtime error otherwise

## Models Parameters
BATCH_SIZE = 32
EPOCHS = 1
LR = 0.0007
WEIGHT_DECAY = 0.0001
LAMBD = "Not_Used"
VGG_LAYERS_BBOX =  [16, "M", 16]
VGG_LAYERS_POSE = [16, "M", 8]
VGG_LAYERS = [64, "M", 64, "M", 128, "M", 128]
RESNET_LAYERS = [3,3,3,3]
MODEL_NUM_LAYERS = 4
MODEL_INPUT_DIM = 128
MODEL_NUM_HEAD = 4
MODEL_MLP_DIM = 256 #deals with FF after the MLA
MODEL_REPRESENTATION_DIM = 512 #deals with FF after the MLA #64 for cnn+lstm
MODEL_NUM_CLASSES = 27  
MODEL_ORIGINAL_INPUT_LENGTH_METHOD_1 = 128+16+16+16+16
MODEL_ORIGINAL_INPUT_LENGTH_METHOD_2 = 128+16+16+16+16 #if method2 is activated

##CROSS ATTENNTION
MODEL_ORIGINAL_INPUT_CA_LENGTH_METHOD_1 = 128+16+16

## Architecture.py - CrossAttentionModel Class should follow the below class
class SelfAttentionViT(nn.Module):
    def __init__(
        self,
        original_input_length,
        num_layers,
        num_heads,
        input_dim,
        mlp_dim,
        representation_dim,
        num_classes,
        device,
        is_ocr_cost_func,
        dropout=0.4,
        attention_dropout=0.4
    ):
        super(SelfAttentionViT, self).__init__()

        self.input_channel = 3

        self.pose_bbox_inputs_dim = 8

        self.device = device

        self.pose_as_input = POSE_AS_INPUT

        self.bbox_as_input = BBOX_AS_INPUT

        self.pose_graph_edge_index = POSE_GRAPH_EDGE_INDEX

        self.is_ocr_cost_func = is_ocr_cost_func
        
        if self.is_ocr_cost_func == True:
            logging.info("OCR as Cost Enabled")
            original_input_length = original_input_length - 16
            #print(original_input_length)

        self.vgg = VGGBlock(
            self.input_channel
        )

        self.left_pose_graph = GATv2Conv(in_channels = 3, out_channels= 8, n_heads=3)

        self.pose_left_avg_pool = nn.AdaptiveAvgPool2d((1, 8))

        # modality FFs
        self.left_pose_ff = nn.Sequential(
            nn.Linear(51, 8),
            nn.ReLU(inplace=True),
        )

        self.right_pose_graph = GATv2Conv(in_channels = 3, out_channels= 8, n_heads=3)

        self.pose_right_avg_pool = nn.AdaptiveAvgPool2d((1, 8))

        self.right_pose_ff = nn.Sequential(
            nn.Linear(51, 8),
            nn.ReLU(inplace=True),
        )
        self.left_gaze_ff = nn.Sequential(
            nn.Linear(3, 8),
            nn.ReLU(inplace=True),
        )
        self.right_gaze_ff = nn.Sequential(
            nn.Linear(3, 8),
            nn.ReLU(inplace=True),
        )

        self.bbox_layers = VGGBlockBbox()

        self.bbox_ff = nn.Sequential(
            nn.Linear(108, 16),
            nn.ReLU(inplace=True),
        )
        self.ocr_ff = nn.Sequential(
            nn.Linear(729, 16),
            nn.ReLU(inplace=True),
        )
        self.projection_mlp = nn.Linear(
            original_input_length, input_dim
        )
        self.encoder = SelfAttentionEncoder(
            num_layers, num_heads, input_dim, mlp_dim, dropout, attention_dropout
        )

        left_heads_layers = OrderedDict()
        left_heads_layers["pre_logits"] = nn.Linear(input_dim, representation_dim)
        left_heads_layers["act"] = nn.ReLU()
        left_heads_layers["head"] = nn.Linear(representation_dim, num_classes)

        right_heads_layers = OrderedDict()
        right_heads_layers["pre_logits"] = nn.Linear(input_dim, representation_dim)
        right_heads_layers["act"] = nn.ReLU()
        right_heads_layers["head"] = nn.Linear(representation_dim, num_classes)

        ocr_head_layers = OrderedDict()
        ocr_head_layers["head"] = nn.Linear(input_dim, 1)
        ocr_head_layers["act"] = nn.Sigmoid()

        self.left_beliefs_heads = nn.Sequential(left_heads_layers)
        self.right_beliefs_heads = nn.Sequential(right_heads_layers)
        self.ocr_head = nn.Sequential(ocr_head_layers)

    def forward(self, images, left_poses, right_poses,  left_gazes, right_gazes, bboxes, ocr_tensor):

        image_feat = self.vgg(images.to(self.device))
        
        if self.pose_as_input == "coordinates":
            left_poses_feat = self.left_pose_ff(left_poses.to(self.device))
            image_feat = torch.cat([image_feat, left_poses_feat], 1)
            #print("image + left pose")
            #print(image_feat.shape)

        elif self.pose_as_input == "graph":
            bs, l, c = left_poses.shape
            left_pose = self.left_pose_graph(left_poses.to(self.device).view(-1, c), self.pose_graph_edge_index.to(self.device))
            left_poses_feat = torch.squeeze(self.pose_left_avg_pool(left_pose.view(bs, l, self.pose_bbox_inputs_dim)))
            image_feat = torch.cat([image_feat, left_poses_feat.to(self.device)], 1)

        if self.pose_as_input == "coordinates":
            right_poses_feat = self.right_pose_ff(right_poses.to(self.device))
            image_feat = torch.cat([image_feat, right_poses_feat], 1)
            #print("image + left and right pose")
            #print(image_feat.shape)

        elif self.pose_as_input == "graph":
            bs, l, c = right_poses.shape
            right_pose = self.right_pose_graph(right_poses.to(self.device).view(-1, c), self.pose_graph_edge_index.to(self.device))
            right_poses_feat = torch.squeeze(self.pose_right_avg_pool(right_pose.view(bs, l, self.pose_bbox_inputs_dim)))
            image_feat = torch.cat([image_feat, right_poses_feat.to(self.device)], 1)

        if left_gazes is not None:
            left_gazes_feat = self.left_gaze_ff(left_gazes.to(self.device))
            image_feat = torch.cat([image_feat, left_gazes_feat], 1)
            #print("image + left and right pose + left gaze")
            #print(image_feat.shape)
        
        if right_gazes is not None:
            right_gazes_feat = self.right_gaze_ff(right_gazes.to(self.device))
            image_feat = torch.cat([image_feat, right_gazes_feat], 1)
            #print("image + left and right pose + left gaze")
            #print(image_feat.shape)

        if self.bbox_as_input == "new_coordinates" or self.bbox_as_input == "old_coordinates":
            ## BBOX AS COORDINATES
            bboxes_feat = self.bbox_ff(bboxes.to(self.device))
            image_feat = torch.cat([image_feat, bboxes_feat], 1)

        elif self.bbox_as_input == "picture":
            bboxes_feat = self.bbox_layers(bboxes.to(self.device))
            image_feat = torch.cat([image_feat, bboxes_feat], 1)

        
        # if self.is_ocr_cost_func == False:
        if not self.is_ocr_cost_func == True:
            ocr_tensor_feat = self.ocr_ff(ocr_tensor.to(self.device))
            image_feat = torch.cat([image_feat, ocr_tensor_feat], 1)

        else:
            image_feat = image_feat


        proj_inp  = self.projection_mlp(image_feat)

        #logging.info("Proj Inp - {}".format(proj_inp))
        self_att_encoder = self.encoder(proj_inp)
        #logging.info("Tansformer Output - {}".format(self_att_encoder))
\
        left_beliefs = self.left_beliefs_heads(self_att_encoder)

        right_beliefs = self.right_beliefs_heads(self_att_encoder)
\
        if self.is_ocr_cost_func == True:
            logging.info("OCR as final layer")
            ocr_out = self.ocr_head(self_att_encoder)
        else:
            ocr_out = 0

        return left_beliefs, right_beliefs, ocr_out